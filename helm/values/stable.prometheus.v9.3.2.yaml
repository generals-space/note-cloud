## 使用方法
## k create ns monitoring
## helm install prometheus -n monitoring -f /home/charts/stable.prometheus.yaml stable/prometheus
## helm upgrade prometheus -n monitoring -f /home/charts/stable.prometheus.yaml stable/prometheus

server:
  enabled: true
  name: server
  enableAdminApi: false

  global:
    scrape_interval: 1m
    scrape_timeout: 10s
    ## How frequently to evaluate rules
    evaluation_interval: 1m

  replicaCount: 1
  statefulSet:
    ## 如果是false则使用deployment部署, 否则用statefulSet部署.
    enabled: false
    ## Alertmanager headless service to use for the statefulset
    headless:
      servicePort: 80
  extraArgs:
    ## 非常重要, 开启热更新支持, 有了这个参数之后, prometheus.yml 配置文件只要更新了, 
    ## 通过执行http://localhost:9090/-/reload就会立即生效, 所以一定要加上这个参数.
    web.enable-lifecycle: true
    web.enable-admin-api: true
  service:
    type: NodePort
    servicePort: 80
    nodePort: 30090
  ingress:
    enabled: false
    hosts: []
    #   - alertmanager.domain.com
    #   - domain.com/alertmanager
    tls: []
    #   - secretName: prometheus-alerts-tls
    #     hosts:
    #       - alertmanager.domain.com
  persistentVolume:
    enabled: true
    storageClass: local-path

## server end ...

alertmanager:
  enabled: true
  replicaCount: 1
  ingress:
    enabled: false

  service:
    type: NodePort
    servicePort: 80
    nodePort: 30180

  persistentVolume:
    ## false会使用emptyDir
    enabled: true
    storageClass: local-path

## alertmanager end ...

kubeStateMetrics:
  enabled: true
  name: kube-state-metrics
  replicaCount: 1
  service:
    type: ClusterIP
    servicePort: 80
## kubeStateMetrics end ...

nodeExporter:
  enabled: true
  name: node-exporter
  hostNetwork: true
  hostPID: true

  service:
    type: ClusterIP
    hostPort: 9100
    servicePort: 9100
## nodeExporter end ...

pushgateway:
  enabled: true
  name: pushgateway
  replicaCount: 1
  service:
    type: ClusterIP
    servicePort: 9091
  ingress:
    enabled: false
  persistentVolume:
    enabled: false
## pushgateway end ...

serverFiles:
  prometheus.yml:
    ## prometheus抓取规则
    scrape_configs:
      - job_name: prometheus
        static_configs:
          - targets:
            - localhost:9090
      ## job prometheus end ...

      - job_name: kubernetes-apiservers
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names: 
                - default
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
          - source_labels: 
              - __meta_kubernetes_namespace
              - __meta_kubernetes_service_name
              - __meta_kubernetes_endpoint_port_name
            action: keep
            regex: default;kubernetes;https
      ## job kubernetes-apiservers end 

      - job_name: kubernetes-services
        ## 使用自动服务发现监控service需要部署 blackbox-exporter 服务.
        ## 见: https://yunlzheng.gitbook.io/prometheus-book/part-iii-prometheus-shi-zhan/readmd/use-prometheus-monitor-kubernetes#dui-ingress-he-service-jin-hang-wang-luo-tan-ce
        metrics_path: /probe
        params:
          module: [http_2xx]
        kubernetes_sd_configs:
          - role: service
        relabel_configs:
          ## `__address__`原为服务发现时获取到的service对象地址, 
          ## 这里将赋值给获取监控数据的请求参数`__param_target`.
          - source_labels: [__address__]
            target_label: __param_target
          - target_label: __address__
            replacement: blackbox-exporter.monitoring.svc.cluster.local:9115
          ## `__param_target`指定了`blackbox_exporter`要探测的目标,
          ## 这里把发现到的service对象中的instance属性值作为目标, 以实现动态检测.
          - source_labels: [__param_target]
            target_label: instance
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_service_name]
            target_label: kubernetes_name
      ## job kubernetes-services end ...
    ## alert end ...

  ## 告警规则
  ## Ref: https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/
  alerts: 
    groups:
      - name: Instances
        rules:
          - alert: InstanceDown
            expr: up == 0
            for: 5m
            labels:
              severity: page
            annotations:
              description: '{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.'
              summary: 'Instance {{ $labels.instance }} down'
      ## rules end ...

      - name: general-service
        rules:
          - alert: ProbeFailing
            expr: up {job = 'kubernetes-services'} == 0 or probe_success {job = 'kubernetes-services'} == 0
            for: 5m
            labels: 
              serverity: Warning
      ## rules end ...

    ## groups end ...
  ## alerts end ...
## serverFiles end ...

## alertmanager报警规则
alertmanagerFiles:
  alertmanager.yml:
    global: 
      ## global 一般用来配置邮件发送方(smtp_xxx等), 微信企业版接口认证, webhook的客户端配置(账号密码)等.

      ## 当prometheus发现故障已经解除后并不会立刻发送恢复通知, 
      ## 而是需要再等待`resolve_time`时间进行确认.
      ## 所以在已经修复故障之后, 最多需要等待`global.resolve_timeout`+`receivers.email_configs.send_resolved`的时间.
      ## 但事实并非如此...实际上这个值好像不会影响恢复通知的发送.
      resolve_timeout: 5m

      ## 邮箱smtp服务器代理
      smtp_smarthost: smtp.qq.com:465
      ## 发送方的邮件地址, smtp服务器以此验证与授权码是否相符
      smtp_auth_username: sender_addr@qq.com
      ## 邮箱密码, 如果是使用qq, 163等, 这里可能需要填写授权码.
      smtp_auth_password: xxxxxxxxx
      ## 邮件发送方的邮箱地址, 也是 email_configs.from 的默认取值.
      smtp_from: sender_addr@qq.com
      ## qq邮箱一般填false
      smtp_require_tls: false

      ## 微信告警配置
      ## wechat_api_corp_id: "企业微信账号唯一ID"
      ## wechat_api_secret: "自定义应用的密钥"
    receivers:
      - name: default-receiver
        email_configs:
          ## email_configs中各字段都可以在global块中找到全局配置作为默认值.
          ## 邮件接收方地址, 如果是企业邮箱, 可以通过分组达到群发的目的.
          - to: receiver_addr@qq.com
            ## 故障恢复后是否通知.
            ## 注意: 故障恢复通知需要在`group_interval`时间前确实有警告发生, 
            ## 且之后`group_interval`时间内没有再发生, 才会发送.
            ## (否则alert-manager重启一次启不是要给所有规则都发一次恢复邮件?)
            ## 但也不是立刻发送通知, 而是需要再继续观察`global.resolve_timeout`进行确认.
            send_resolved: true
    route:
      receiver: default-receiver
      ## 最初(即第一次)等待多久时间发送一组警报的通知
      group_wait: 10s
      ## 告警邮件(或是其他类型的通知)发送的频率
      group_interval: 5m
      ## 发送警报的周期
      repeat_interval: 1m
    ## 自定义通知模板(邮件/微信等)路径, 可使用通配符.
    templates: 
      - /etc/config/email_subject.tmpl

  ## alertmanager.yml end ...

  ## 自定义模板
  email_subject.tmpl: |
    {{ define "email.custome.subject" -}}
    [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .GroupLabels.SortedPairs.Values | join " " }} {{ if gt (len .CommonLabels) (len .GroupLabels) }}({{ with .CommonLabels.Remove .GroupLabels.Names }}{{ .Values | join " " }}{{ end }}){{ end }}
    {{- end}}

## alertmanagerFiles end ...
